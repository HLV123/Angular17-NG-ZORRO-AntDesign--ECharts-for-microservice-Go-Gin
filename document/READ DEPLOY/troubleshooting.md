# Maintenix â€” Troubleshooting Guide

> **Smart Predictive Maintenance Warning System**
> HÆ°á»›ng dáº«n cháº©n Ä‘oÃ¡n vÃ  kháº¯c phá»¥c sá»± cá»‘ toÃ n diá»‡n: cÃ i Ä‘áº·t mÃ´i trÆ°á»ng, Docker infrastructure, backend services, frontend, database, Kafka, gRPC, networking, performance, production.

---

## Má»¥c lá»¥c

1. [Quy trÃ¬nh Cháº©n Ä‘oÃ¡n Tá»•ng quÃ¡t](#1-quy-trÃ¬nh-cháº©n-Ä‘oÃ¡n-tá»•ng-quÃ¡t)
2. [CÃ i Ä‘áº·t MÃ´i trÆ°á»ng â€” Windows](#2-cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng--windows)
3. [Docker & Docker Compose](#3-docker--docker-compose)
4. [PostgreSQL & TimescaleDB](#4-postgresql--timescaledb)
5. [InfluxDB](#5-influxdb)
6. [Redis](#6-redis)
7. [Apache Kafka](#7-apache-kafka)
8. [Backend â€” Go Services](#8-backend--go-services)
9. [Backend â€” gRPC](#9-backend--grpc)
10. [Backend â€” API Gateway](#10-backend--api-gateway)
11. [Frontend â€” Angular](#11-frontend--angular)
12. [Authentication & Authorization (RBAC)](#12-authentication--authorization-rbac)
13. [WebSocket & Real-time Data](#13-websocket--real-time-data)
14. [Sensor Pipeline & Anomaly Detection](#14-sensor-pipeline--anomaly-detection)
15. [AI/ML Service](#15-aiml-service)
16. [Monitoring & Observability](#16-monitoring--observability)
17. [Performance & Tá»‘i Æ°u](#17-performance--tá»‘i-Æ°u)
18. [Networking & Port Conflicts](#18-networking--port-conflicts)
19. [Data & Migration Issues](#19-data--migration-issues)
20. [Production Issues](#20-production-issues)
21. [Quick Reference â€” Lá»‡nh Cháº©n Ä‘oÃ¡n](#21-quick-reference--lá»‡nh-cháº©n-Ä‘oÃ¡n)

---

## 1. Quy trÃ¬nh Cháº©n Ä‘oÃ¡n Tá»•ng quÃ¡t

### 1.1. Flowchart Cháº©n Ä‘oÃ¡n

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚  PhÃ¡t hiá»‡n lá»—i   â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  XÃ¡c Ä‘á»‹nh pháº¡m vi lá»—i   â”‚
                    â”‚  (Frontend / Backend /  â”‚
                    â”‚   Infrastructure / All) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                  â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Frontend     â”‚  â”‚  Backend      â”‚  â”‚ Infrastructure â”‚
     â”‚  (Browser)    â”‚  â”‚  (Services)   â”‚  â”‚  (Docker)      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                  â”‚                   â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ 1. DevTools   â”‚  â”‚ 1. Service    â”‚  â”‚ 1. docker ps   â”‚
     â”‚    Console    â”‚  â”‚    logs       â”‚  â”‚ 2. docker logs â”‚
     â”‚ 2. Network tabâ”‚  â”‚ 2. Health     â”‚  â”‚ 3. Resource    â”‚
     â”‚ 3. ng serve   â”‚  â”‚    endpoint   â”‚  â”‚    usage       â”‚
     â”‚    output     â”‚  â”‚ 3. Debug      â”‚  â”‚ 4. Network     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    endpoint   â”‚  â”‚    inspect     â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Kiá»ƒm tra logs chi tiáº¿tâ”‚
                    â”‚   â†’ TÃ¬m error code      â”‚
                    â”‚   â†’ Tra báº£ng bÃªn dÆ°á»›i   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      Ãp dá»¥ng fix        â”‚
                    â”‚   â†’ Verify â†’ Document   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2. Kiá»ƒm tra Nhanh Sá»©c khá»e Há»‡ thá»‘ng

```bash
# ===== 1. Infrastructure healthy? =====
docker compose -f docker-compose.infra.yml ps
# Táº¥t cáº£ containers pháº£i á»Ÿ tráº¡ng thÃ¡i "Up (healthy)"

# ===== 2. Backend services running? =====
curl -s http://localhost:8080/health | jq .
# Mong Ä‘á»£i: {"status":"ok","services":{...}}

# Kiá»ƒm tra tá»«ng service
for port in 8081 8082 8083 8084 8085 8086 8087; do
  echo -n "Service :${port} â†’ "
  curl -s --max-time 2 http://localhost:${port}/health | jq -r '.status' 2>/dev/null || echo "DOWN"
done

# ===== 3. Frontend running? =====
curl -s --max-time 2 http://localhost:4200 > /dev/null && echo "Frontend: UP" || echo "Frontend: DOWN"

# ===== 4. Database connections? =====
docker exec maintenix-postgres psql -U maintenix -c "SELECT 1;" > /dev/null 2>&1 && echo "PostgreSQL: OK" || echo "PostgreSQL: FAIL"
docker exec maintenix-redis redis-cli PING 2>/dev/null || echo "Redis: FAIL"

# ===== 5. Kafka healthy? =====
docker exec maintenix-kafka kafka-topics.sh --list --bootstrap-server localhost:9092 > /dev/null 2>&1 && echo "Kafka: OK" || echo "Kafka: FAIL"
```

### 1.3. Severity Levels

| Level | KÃ½ hiá»‡u | MÃ´ táº£ | VÃ­ dá»¥ |
|-------|---------|-------|-------|
| **P0 â€” Critical** | ğŸ”´ | Há»‡ thá»‘ng down hoÃ n toÃ n, khÃ´ng sá»­ dá»¥ng Ä‘Æ°á»£c | Táº¥t cáº£ services crash, DB corruption |
| **P1 â€” High** | ğŸŸ  | Feature chÃ­nh khÃ´ng hoáº¡t Ä‘á»™ng | Alert pipeline dá»«ng, khÃ´ng login Ä‘Æ°á»£c |
| **P2 â€” Medium** | ğŸŸ¡ | Feature phá»¥ lá»—i, workaround cÃ³ | Report export fail, WebSocket ngáº¯t káº¿t ná»‘i |
| **P3 â€” Low** | ğŸŸ¢ | Cosmetic, khÃ´ng áº£nh hÆ°á»Ÿng chá»©c nÄƒng | UI hiá»ƒn thá»‹ sai format, log verbose |

---

## 2. CÃ i Ä‘áº·t MÃ´i trÆ°á»ng â€” Windows

### 2.1. Lá»—i cÃ i Ä‘áº·t Tools

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `'node' is not recognized as an internal or external command` | Node.js chÆ°a thÃªm vÃ o PATH | CÃ i láº¡i Node.js báº±ng `winget install OpenJS.NodeJS.LTS`, tick "Add to PATH". Restart terminal |
| 2 | `'ng' is not recognized...` | Angular CLI chÆ°a cÃ i global | `npm install -g @angular/cli`. Náº¿u váº«n lá»—i: kiá»ƒm tra `npm prefix -g` cÃ³ trong PATH khÃ´ng |
| 3 | `'go' is not recognized...` | Go chÆ°a cÃ i hoáº·c PATH sai | Kiá»ƒm tra `C:\Program Files\Go\bin` trong System PATH. Restart terminal sau khi thÃªm |
| 4 | `go install ... permission denied` | Thiáº¿u quyá»n write vÃ o `$GOPATH/bin` | Cháº¡y PowerShell as Administrator. Hoáº·c set `$env:GOPATH = "$env:USERPROFILE\go"` |
| 5 | `protoc: command not found` | protoc chÆ°a cÃ i hoáº·c PATH sai | `choco install protoc -y`. Kiá»ƒm tra `C:\ProgramData\chocolatey\bin` trong PATH |
| 6 | `npm install` ráº¥t cháº­m / timeout | Registry cháº­m tá»« Viá»‡t Nam | DÃ¹ng mirror: `npm config set registry https://registry.npmmirror.com`. Hoáº·c dÃ¹ng VPN |
| 7 | `EPERM: operation not permitted` khi npm install | File bá»‹ lock (VSCode, antivirus) | ÄÃ³ng VSCode â†’ xÃ³a `node_modules` â†’ cháº¡y láº¡i `npm install` |
| 8 | `The term 'make' is not recognized` | GNU Make chÆ°a cÃ i trÃªn Windows | `choco install make -y`. Alternative: dÃ¹ng `go-task` (`choco install go-task -y`) |
| 9 | `air: command not found` | Go tools chÆ°a trong PATH | ThÃªm `$env:GOPATH\bin` (thÆ°á»ng `C:\Users\<user>\go\bin`) vÃ o PATH. Verify: `go env GOPATH` |
| 10 | `WSL2 is not installed` khi má»Ÿ Docker Desktop | WSL2 chÆ°a báº­t | PowerShell Admin: `wsl --install` â†’ restart â†’ `wsl --set-default-version 2` |

### 2.2. Kiá»ƒm tra mÃ´i trÆ°á»ng Ä‘áº§y Ä‘á»§

```powershell
# Script kiá»ƒm tra táº¥t cáº£ tools cáº§n thiáº¿t
$tools = @(
    @{ Name="Git";      Cmd="git --version" },
    @{ Name="Node.js";  Cmd="node --version" },
    @{ Name="npm";      Cmd="npm --version" },
    @{ Name="Angular";  Cmd="ng version 2>&1 | Select-String 'Angular CLI'" },
    @{ Name="Go";       Cmd="go version" },
    @{ Name="Docker";   Cmd="docker --version" },
    @{ Name="Compose";  Cmd="docker compose version" },
    @{ Name="Make";     Cmd="make --version 2>&1 | Select-Object -First 1" },
    @{ Name="protoc";   Cmd="protoc --version" },
    @{ Name="air";      Cmd="air -v 2>&1 | Select-Object -First 1" },
    @{ Name="golangci"; Cmd="golangci-lint --version 2>&1 | Select-Object -First 1" }
)

foreach ($tool in $tools) {
    try {
        $result = Invoke-Expression $tool.Cmd 2>$null
        if ($result) {
            Write-Host "âœ… $($tool.Name): $result" -ForegroundColor Green
        } else {
            Write-Host "âŒ $($tool.Name): Not found" -ForegroundColor Red
        }
    } catch {
        Write-Host "âŒ $($tool.Name): Not found" -ForegroundColor Red
    }
}
```

---

## 3. Docker & Docker Compose

### 3.1. Docker Desktop Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Docker Desktop khÃ´ng khá»Ÿi Ä‘á»™ng | WSL2 chÆ°a báº­t hoáº·c Hyper-V conflict | Báº­t WSL2: `wsl --install`. Táº¯t Hyper-V náº¿u dÃ¹ng WSL2 backend. Restart mÃ¡y |
| 2 | `Cannot connect to Docker daemon` | Docker service chÆ°a start | Má»Ÿ Docker Desktop GUI. Hoáº·c PowerShell Admin: `Start-Service Docker` |
| 3 | `docker compose: command not found` | Docker Compose plugin chÆ°a cÃ i | Cáº­p nháº­t Docker Desktop (Compose V2 built-in). Hoáº·c: `docker-compose` (V1 legacy) |
| 4 | Container restart loop (`Restarting`) | Dependency chÆ°a ready hoáº·c config sai | `docker compose logs <service>` â†’ Ä‘á»c error. Kiá»ƒm tra healthcheck dependencies |
| 5 | `OCI runtime create failed` | Image corrupted hoáº·c thiáº¿u | `docker compose pull` Ä‘á»ƒ táº£i láº¡i images. Hoáº·c `docker system prune` |
| 6 | MÃ¡y ráº¥t cháº­m khi cháº¡y Docker | Docker dÃ¹ng quÃ¡ nhiá»u RAM/CPU | Docker Desktop â†’ Settings â†’ Resources: giá»›i háº¡n Memory â‰¤ 8GB, CPUs â‰¤ 4 |
| 7 | `no space left on device` | Docker volumes/images chiáº¿m háº¿t á»• cá»©ng | `docker system prune -a --volumes` (âš ï¸ xÃ³a táº¥t cáº£ data). Hoáº·c xÃ³a cÃ³ chá»n lá»c bÃªn dÆ°á»›i |
| 8 | Build ráº¥t cháº­m | KhÃ´ng dÃ¹ng cache layer | Kiá»ƒm tra `Dockerfile` cÃ³ multi-stage build. DÃ¹ng `docker compose build --parallel` |

### 3.2. Docker Compose â€” Infrastructure

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `port is already allocated` | Port bá»‹ chiáº¿m bá»Ÿi process khÃ¡c | TÃ¬m process: `netstat -ano \| findstr :<PORT>` â†’ `taskkill /PID <PID>`. Hoáº·c Ä‘á»•i port trong docker-compose |
| 2 | Kafka khÃ´ng start, log "Cluster ID mismatch" | Data volume cÅ© conflict | `docker compose down -v` rá»“i `docker compose up -d` (xÃ³a volumes) |
| 3 | Container `unhealthy` | Service bÃªn trong chÆ°a ready | Kiá»ƒm tra logs: `docker compose logs <service>`. TÄƒng `start_period` trong healthcheck |
| 4 | `network maintenix-network not found` | Network chÆ°a Ä‘Æ°á»£c táº¡o | `docker compose down` rá»“i `docker compose up -d` (táº¡o láº¡i network) |
| 5 | Service A khÃ´ng káº¿t ná»‘i Service B | Sai hostname hoáº·c khÃ¡c network | Trong Docker network, dÃ¹ng container name (vd: `postgres`, `redis`), KHÃ”NG dÃ¹ng `localhost` |
| 6 | Volume mount permission denied | Quyá»n file trÃªn host khÃ¡c container | Linux: `chmod -R 777 <volume-dir>`. Docker Desktop: Settings â†’ File Sharing â†’ thÃªm thÆ° má»¥c |

### 3.3. Dá»n dáº¹p Docker Resources

```bash
# Xem disk usage
docker system df

# XÃ³a containers Ä‘Ã£ dá»«ng + images khÃ´ng dÃ¹ng + networks khÃ´ng dÃ¹ng
docker system prune

# XÃ³a táº¥t cáº£ (bao gá»“m volumes â€” Máº¤T DATA)
docker system prune -a --volumes

# XÃ³a cÃ³ chá»n lá»c
docker image prune -a                     # XÃ³a images khÃ´ng dÃ¹ng
docker volume prune                       # XÃ³a volumes orphan
docker container prune                    # XÃ³a containers Ä‘Ã£ dá»«ng
docker builder prune                      # XÃ³a build cache

# XÃ³a chá»‰ volumes cá»§a Maintenix (reset data)
docker compose -f docker-compose.infra.yml down -v

# Restart sáº¡ch toÃ n bá»™ infrastructure
docker compose -f docker-compose.infra.yml down -v --remove-orphans
docker compose -f docker-compose.infra.yml up -d
```

---

## 4. PostgreSQL & TimescaleDB

### 4.1. Connection Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `connection refused (port 5432)` | PostgreSQL chÆ°a start hoáº·c sai port | `docker compose ps postgres` kiá»ƒm tra status. Verify: `docker exec maintenix-postgres pg_isready` |
| 2 | `password authentication failed` | Sai credentials | Kiá»ƒm tra `.env` file: `POSTGRES_USER=maintenix`, `POSTGRES_PASSWORD=secret`. Náº¿u Ä‘á»•i password, pháº£i xÃ³a volume vÃ  táº¡o láº¡i |
| 3 | `FATAL: database "maintenix" does not exist` | DB chÆ°a Ä‘Æ°á»£c táº¡o hoáº·c init script fail | `docker exec maintenix-postgres psql -U maintenix -c "CREATE DATABASE maintenix;"`. Hoáº·c kiá»ƒm tra init script |
| 4 | `too many connections` | Connection pool exhausted | TÄƒng `max_connections` trong PostgreSQL config. Kiá»ƒm tra connection leak trong Go code (defer `db.Close()`) |
| 5 | `could not connect to server: Connection timed out` | Network issue hoáº·c firewall | Kiá»ƒm tra container network: `docker network inspect maintenix-network`. Verify port mapping |
| 6 | TimescaleDB: `extension "timescaledb" is not available` | Extension chÆ°a cÃ i | Kiá»ƒm tra image Ä‘Ãºng `timescale/timescaledb:latest-pg16`. Cháº¡y: `CREATE EXTENSION IF NOT EXISTS timescaledb;` |
| 7 | Query ráº¥t cháº­m | Thiáº¿u index hoáº·c data quÃ¡ lá»›n | Cháº¡y `EXPLAIN ANALYZE <query>` â†’ thÃªm index phÃ¹ há»£p. TimescaleDB: kiá»ƒm tra hypertable chunk interval |
| 8 | `relation "xxx" does not exist` | Migration chÆ°a cháº¡y | `make migrate-up` hoáº·c `migrate -path migrations -database $DATABASE_URL up` |

### 4.2. Debug Commands â€” PostgreSQL

```bash
# Káº¿t ná»‘i vÃ o PostgreSQL
docker exec -it maintenix-postgres psql -U maintenix

# Kiá»ƒm tra databases
docker exec maintenix-postgres psql -U maintenix -c "\l"

# Kiá»ƒm tra tables
docker exec maintenix-postgres psql -U maintenix -c "\dt"

# Kiá»ƒm tra connections Ä‘ang má»Ÿ
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT count(*), state FROM pg_stat_activity GROUP BY state;"

# Kill connections Ä‘ang stuck
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE state = 'idle in transaction' AND query_start < now() - interval '5 minutes';"

# Kiá»ƒm tra table sizes
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT relname, pg_size_pretty(pg_total_relation_size(relid)) FROM pg_catalog.pg_statio_user_tables ORDER BY pg_total_relation_size(relid) DESC LIMIT 10;"

# Kiá»ƒm tra migration version
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT * FROM schema_migrations ORDER BY version DESC LIMIT 5;"

# TimescaleDB: Kiá»ƒm tra hypertables
docker exec maintenix-timescaledb psql -U maintenix -d maintenix_ts -c \
  "SELECT hypertable_name, num_chunks, approximate_row_count(format('%I.%I', hypertable_schema, hypertable_name)::regclass) FROM timescaledb_information.hypertables;"

# TimescaleDB: Kiá»ƒm tra chunk sizes
docker exec maintenix-timescaledb psql -U maintenix -d maintenix_ts -c \
  "SELECT chunk_name, range_start, range_end, pg_size_pretty(total_bytes) FROM timescaledb_information.chunks ORDER BY range_end DESC LIMIT 10;"
```

---

## 5. InfluxDB

### 5.1. Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `401 Unauthorized` | Token sai | Kiá»ƒm tra `INFLUX_TOKEN` trong `.env`. Default: `maintenix-influx-token`. Hoáº·c táº¡o token má»›i qua UI http://localhost:8086 |
| 2 | `bucket not found` | Bucket chÆ°a táº¡o hoáº·c sai tÃªn | UI: http://localhost:8086 â†’ Buckets â†’ táº¡o `sensor_realtime`. Hoáº·c: `influx bucket create -n sensor_realtime -o maintenix-org` |
| 3 | Write timeout | InfluxDB quÃ¡ táº£i hoáº·c disk slow | Kiá»ƒm tra `docker stats maintenix-influxdb`. TÄƒng resource limits. Giáº£m write batch size |
| 4 | Query tráº£ káº¿t quáº£ rá»—ng | Sai time range hoáº·c measurement name | Kiá»ƒm tra data: `from(bucket:"sensor_realtime") \|> range(start: -1h) \|> limit(n:5)`. ChÃº Ã½ timezone |
| 5 | Disk usage tÄƒng nhanh | Retention policy khÃ´ng Ä‘Ãºng | Kiá»ƒm tra: `influx bucket list`. Set retention: `influx bucket update -id <ID> -r 168h` (7 ngÃ y) |

### 5.2. Debug Commands â€” InfluxDB

```bash
# Query data gáº§n nháº¥t
docker exec maintenix-influxdb influx query \
  'from(bucket:"sensor_realtime") |> range(start: -1h) |> count()' \
  --org maintenix-org --token maintenix-influx-token

# Kiá»ƒm tra buckets
docker exec maintenix-influxdb influx bucket list \
  --org maintenix-org --token maintenix-influx-token

# Kiá»ƒm tra write throughput
docker exec maintenix-influxdb influx query \
  'from(bucket:"_monitoring") |> range(start: -5m) |> filter(fn: (r) => r._measurement == "write")' \
  --org maintenix-org --token maintenix-influx-token

# XÃ³a data cÅ© (náº¿u disk full)
docker exec maintenix-influxdb influx delete \
  --bucket sensor_realtime --start 2020-01-01T00:00:00Z --stop 2025-01-01T00:00:00Z \
  --org maintenix-org --token maintenix-influx-token
```

---

## 6. Redis

### 6.1. Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `WRONGPASS` hoáº·c `NOAUTH` | Redis cÃ³ password nhÆ°ng client khÃ´ng gá»­i | Kiá»ƒm tra Redis config cÃ³ `requirepass` khÃ´ng. Set `REDIS_PASSWORD` trong `.env` |
| 2 | `OOM command not allowed` | Redis háº¿t memory | Kiá»ƒm tra `maxmemory` config. TÄƒng limit hoáº·c set eviction policy: `maxmemory-policy allkeys-lru` |
| 3 | Cache miss liÃªn tá»¥c (hit rate tháº¥p) | TTL quÃ¡ ngáº¯n hoáº·c key pattern sai | Kiá»ƒm tra TTL: `redis-cli TTL <key>`. Review caching strategy â€” tÄƒng TTL náº¿u data Ã­t thay Ä‘á»•i |
| 4 | Session máº¥t sau khi restart Redis | Redis persistence chÆ°a báº­t | Báº­t AOF: thÃªm `--appendonly yes` vÃ o Docker command. Hoáº·c dÃ¹ng RDB snapshot |
| 5 | `Connection refused` | Redis container chÆ°a healthy | `docker compose ps redis`. Kiá»ƒm tra logs: `docker compose logs redis` |
| 6 | Káº¿t ná»‘i cháº­m / high latency | QuÃ¡ nhiá»u keys hoáº·c slow commands | Kiá»ƒm tra: `redis-cli SLOWLOG GET 10`. TrÃ¡nh `KEYS *` â€” dÃ¹ng `SCAN` thay tháº¿ |

### 6.2. Debug Commands â€” Redis

```bash
# Káº¿t ná»‘i Redis CLI
docker exec -it maintenix-redis redis-cli

# Kiá»ƒm tra thÃ´ng tin tá»•ng quan
docker exec maintenix-redis redis-cli INFO server
docker exec maintenix-redis redis-cli INFO memory
docker exec maintenix-redis redis-cli INFO keyspace

# Kiá»ƒm tra memory usage
docker exec maintenix-redis redis-cli INFO memory | grep used_memory_human

# Liá»‡t kÃª keys theo pattern (dÃ¹ng SCAN, khÃ´ng dÃ¹ng KEYS)
docker exec maintenix-redis redis-cli --scan --pattern "sensor:latest:*" --count 100

# Kiá»ƒm tra session data
docker exec maintenix-redis redis-cli GET "session:<session-id>"

# Kiá»ƒm tra JWT blacklist
docker exec maintenix-redis redis-cli SISMEMBER "jwt:blacklist" "<token-hash>"

# Kiá»ƒm tra cache equipment
docker exec maintenix-redis redis-cli GET "equipment:cache:EQ001"

# Monitor commands real-time (debug â€” táº¯t sau khi dÃ¹ng xong)
docker exec maintenix-redis redis-cli MONITOR

# Kiá»ƒm tra slow queries
docker exec maintenix-redis redis-cli SLOWLOG GET 10

# Flush táº¥t cáº£ cache (âš ï¸ DEV ONLY â€” users pháº£i login láº¡i)
docker exec maintenix-redis redis-cli FLUSHALL
```

---

## 7. Apache Kafka

### 7.1. Kafka Broker Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Kafka khÃ´ng start, log "Cluster ID mismatch" | Volume data cÅ© khÃ´ng khá»›p config má»›i | `docker compose down -v` rá»“i `up -d` (xÃ³a Kafka data) |
| 2 | `LEADER_NOT_AVAILABLE` | Broker chÆ°a hoÃ n táº¥t election | Äá»£i 30-60 giÃ¢y. Náº¿u kÃ©o dÃ i: restart Kafka container |
| 3 | `Topic not found` | Topic chÆ°a Ä‘Æ°á»£c táº¡o | Cháº¡y init script: `./scripts/create-kafka-topics.sh`. Hoáº·c táº¡o thá»§ cÃ´ng â€” xem lá»‡nh bÃªn dÆ°á»›i |
| 4 | Consumer lag tÄƒng liÃªn tá»¥c | Consumer xá»­ lÃ½ cháº­m hÆ¡n producer | TÄƒng partitions cho topic. ThÃªm consumer instances. Optimize consumer logic |
| 5 | `Message too large` | Message vÆ°á»£t `max.message.bytes` | TÄƒng `max.message.bytes` trong broker config. Hoáº·c compress messages (gzip/snappy) |
| 6 | `OFFSET_OUT_OF_RANGE` | Consumer offset Ä‘Ã£ bá»‹ xÃ³a (retention) | Set `auto.offset.reset=earliest` hoáº·c `latest`. Hoáº·c reset offset thá»§ cÃ´ng |
| 7 | Messages vÃ o DLQ | Consumer xá»­ lÃ½ fail sau max retries | Kiá»ƒm tra root cause: `docker exec maintenix-kafka kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic maintenix.dlq.<topic> --from-beginning --max-messages 5` |
| 8 | Kafka UI (http://localhost:9093) khÃ´ng load | Kafka container chÆ°a healthy | Äá»£i Kafka healthy trÆ°á»›c. Kiá»ƒm tra: `docker compose logs kafka-ui` |

### 7.2. Kafka Consumer/Producer Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Producer: `context deadline exceeded` | Kafka broker unreachable hoáº·c quÃ¡ táº£i | Kiá»ƒm tra broker health. TÄƒng `produce.timeout`. Kiá»ƒm tra network |
| 2 | Consumer: khÃ´ng nháº­n messages | Sai consumer group hoáº·c topic | Verify group: `kafka-consumer-groups.sh --describe --group <CG>`. Kiá»ƒm tra topic name |
| 3 | Duplicate messages | Consumer crash trÆ°á»›c khi commit offset | Implement idempotent processing (dedup key trong Redis). Kiá»ƒm tra `enable.auto.commit` |
| 4 | Messages máº¥t thá»© tá»± | Nhiá»u partitions + round-robin produce | Äáº£m báº£o dÃ¹ng partition key (vd: `equipmentId`) cho ordering per entity |
| 5 | Consumer rebalancing liÃªn tá»¥c | `session.timeout.ms` quÃ¡ nhá» hoáº·c consumer quÃ¡ cháº­m | TÄƒng `session.timeout.ms` (30s). TÄƒng `max.poll.interval.ms` (300s) |

### 7.3. Debug Commands â€” Kafka

```bash
# Liá»‡t kÃª topics
docker exec maintenix-kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# Chi tiáº¿t topic
docker exec maintenix-kafka kafka-topics.sh --describe \
  --topic maintenix.sensor.raw --bootstrap-server localhost:9092

# Xem messages má»›i nháº¥t
docker exec maintenix-kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic maintenix.sensor.raw \
  --from-beginning --max-messages 5

# Liá»‡t kÃª consumer groups
docker exec maintenix-kafka kafka-consumer-groups.sh --list --bootstrap-server localhost:9092

# Kiá»ƒm tra consumer lag
docker exec maintenix-kafka kafka-consumer-groups.sh \
  --describe --group sensor-service-cg --bootstrap-server localhost:9092

# Reset consumer offset (âš ï¸ DANGEROUS â€” chá»‰ dÃ¹ng khi cáº§n reprocess)
docker exec maintenix-kafka kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group alert-sensor-processor \
  --topic maintenix.sensor.processed \
  --reset-offsets --to-earliest --execute

# Táº¡o topic thá»§ cÃ´ng
docker exec maintenix-kafka kafka-topics.sh --create \
  --topic maintenix.sensor.raw \
  --partitions 12 --replication-factor 1 \
  --bootstrap-server localhost:9092

# Kiá»ƒm tra DLQ messages
docker exec maintenix-kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic maintenix.dlq.sensor.raw \
  --from-beginning --max-messages 10 \
  --property print.key=true \
  --property print.headers=true

# Replay DLQ messages (custom script)
# go run cmd/dlq-replay/main.go --topic maintenix.dlq.sensor.processed --limit 100
```

---

## 8. Backend â€” Go Services

### 8.1. Build & Compile Errors

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `cannot find package "maintenix/..."` | Module path sai hoáº·c go.mod chÆ°a Ä‘Ãºng | `go mod tidy`. Kiá»ƒm tra `module` name trong `go.mod` |
| 2 | `undefined: SomeFunction` | Import thiáº¿u hoáº·c function chÆ°a export (lowercase) | Kiá»ƒm tra package import. Go: functions pháº£i viáº¿t HOA chá»¯ cÃ¡i Ä‘áº§u Ä‘á»ƒ export |
| 3 | `multiple-value X() in single-value context` | ChÆ°a handle error return | Go functions thÆ°á»ng tráº£ `(value, error)`. Pháº£i handle cáº£ hai: `val, err := fn()` |
| 4 | `go.sum mismatch` hoáº·c checksum error | Module cache corrupted | `go clean -modcache && go mod download` |
| 5 | Build quÃ¡ cháº­m | CGO enabled hoáº·c thiáº¿u cache | `CGO_ENABLED=0 go build ...`. Äáº£m báº£o Go build cache: `go env GOCACHE` |
| 6 | `air` khÃ´ng hot reload | File watcher khÃ´ng detect thay Ä‘á»•i | Kiá»ƒm tra `.air.toml` config. Windows: cÃ³ thá»ƒ cáº§n `poll = true` do filesystem events |

### 8.2. Runtime Errors

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `panic: runtime error: nil pointer dereference` | DÃ¹ng pointer nil chÆ°a check | ThÃªm nil check trÆ°á»›c khi dÃ¹ng pointer. Review error handling flow |
| 2 | `context deadline exceeded` | Request timeout (default 30s) | TÄƒng timeout trong Gin config. Kiá»ƒm tra downstream service latency |
| 3 | `connection refused` khi gá»i service khÃ¡c | Service chÆ°a start hoáº·c sai URL | Kiá»ƒm tra service order (auth-service pháº£i start Ä‘áº§u tiÃªn). Verify URL trong config |
| 4 | `GORM: record not found` | Query tráº£ rá»—ng nhÆ°ng code khÃ´ng handle | DÃ¹ng `errors.Is(err, gorm.ErrRecordNotFound)` â†’ return 404 thay vÃ¬ 500 |
| 5 | `concurrent map writes` / race condition | Map truy cáº­p Ä‘á»“ng thá»i khÃ´ng cÃ³ lock | DÃ¹ng `sync.RWMutex` hoáº·c `sync.Map`. Cháº¡y `go test -race` Ä‘á»ƒ detect |
| 6 | Memory leak â€” RAM tÄƒng liÃªn tá»¥c | Goroutine leak hoáº·c connection khÃ´ng close | Profile: `go tool pprof http://localhost:<port>/debug/pprof/heap`. Kiá»ƒm tra `defer Close()` |
| 7 | Service crash khi Kafka down | KhÃ´ng handle Kafka connection error | Implement retry with backoff. DÃ¹ng circuit breaker cho Kafka producer |
| 8 | `bind: address already in use` | Port Ä‘Ã£ bá»‹ chiáº¿m | `lsof -i :<port>` hoáº·c `netstat -ano | findstr :<port>` â†’ kill process. Hoáº·c Ä‘á»•i port |

### 8.3. Debug Backend Service

```bash
# Xem logs (náº¿u cháº¡y báº±ng air)
# Logs output trá»±c tiáº¿p trong terminal Ä‘ang cháº¡y air

# Health check
curl -s http://localhost:8082/health | jq .

# Debug pprof endpoints (náº¿u Ä‘Ã£ enable)
# CPU profile (30 giÃ¢y)
go tool pprof http://localhost:8082/debug/pprof/profile?seconds=30

# Heap profile
go tool pprof http://localhost:8082/debug/pprof/heap

# Goroutine dump
curl -s http://localhost:8082/debug/pprof/goroutine?debug=2

# Xem active goroutines count
curl -s http://localhost:8082/debug/pprof/goroutine?debug=0 | head -1

# Test API endpoint trá»±c tiáº¿p
curl -s -X POST http://localhost:8081/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"123456"}' | jq .

# Láº¥y token rá»“i test endpoint khÃ¡c
TOKEN=$(curl -s -X POST http://localhost:8081/api/auth/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"123456"}' | jq -r '.data.token')

curl -s http://localhost:8082/api/equipment \
  -H "Authorization: Bearer $TOKEN" | jq .
```

---

## 9. Backend â€” gRPC

### 9.1. gRPC Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `transport: Error while dialing: connection refused` | gRPC server chÆ°a start | Kiá»ƒm tra service log. Verify gRPC port (50051, 50052, ...) Ä‘ang listen |
| 2 | `rpc error: code = Unavailable` | Server unreachable hoáº·c circuit breaker open | Kiá»ƒm tra server health. Circuit breaker reset sau 30s (half-open mode) |
| 3 | `rpc error: code = Unauthenticated` | Token sai hoáº·c háº¿t háº¡n | Validate token: `grpcurl -plaintext -d '{"token":"<JWT>"}' localhost:50051 auth.v1.AuthService/ValidateToken` |
| 4 | `rpc error: code = DeadlineExceeded` | gRPC call timeout (default 5s) | TÄƒng deadline trong client config. Kiá»ƒm tra server processing time |
| 5 | `protoc-gen-go: program not found` | gRPC Go plugins chÆ°a cÃ i | `go install google.golang.org/protobuf/cmd/protoc-gen-go@latest` vÃ  `go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest` |
| 6 | Proto generate ra code lá»—i | Proto file syntax sai | Validate: `buf lint`. Check `option go_package` trong .proto file |
| 7 | `stream terminated by RST_STREAM` | HTTP/2 connection bá»‹ reset | Kiá»ƒm tra proxy (Nginx) cÃ³ support gRPC khÃ´ng. Cáº§n `grpc_pass` thay vÃ¬ `proxy_pass` |

### 9.2. Debug gRPC

```bash
# Liá»‡t kÃª services
grpcurl -plaintext localhost:50051 list

# Liá»‡t kÃª methods
grpcurl -plaintext localhost:50051 list auth.v1.AuthService

# Describe service
grpcurl -plaintext localhost:50051 describe auth.v1.AuthService

# Gá»i method
grpcurl -plaintext -d '{"token":"eyJhbGciOi..."}' \
  localhost:50051 auth.v1.AuthService/ValidateToken

# Gá»i method qua equipment service
grpcurl -plaintext -d '{"equipment_id":"EQ001"}' \
  localhost:50052 equipment.v1.EquipmentService/GetHealthScore

# Stream sensor data
grpcurl -plaintext -d '{"sensor_id":"S001","interval_seconds":5}' \
  localhost:50053 sensor.v1.SensorStreamService/StreamSensorData
```

---

## 10. Backend â€” API Gateway

### 10.1. API Gateway Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `502 Bad Gateway` | Downstream service down | Kiá»ƒm tra service tÆ°Æ¡ng á»©ng: `curl http://localhost:<port>/health`. Restart service náº¿u cáº§n |
| 2 | `504 Gateway Timeout` | Downstream service xá»­ lÃ½ quÃ¡ cháº­m | TÄƒng timeout trong Nginx config. Kiá»ƒm tra DB query performance |
| 3 | `413 Request Entity Too Large` | Upload file quÃ¡ lá»›n | TÄƒng `client_max_body_size` trong Nginx. Default: 1MB â†’ set 50MB |
| 4 | `CORS error` (browser console) | CORS headers thiáº¿u | Kiá»ƒm tra CORS config trong gateway. Äáº£m báº£o `Access-Control-Allow-Origin` cho `http://localhost:4200` |
| 5 | `429 Too Many Requests` | Rate limit triggered | Giáº£m request frequency. Hoáº·c tÄƒng rate limit config (dev mode) |
| 6 | `403 Forbidden` â€” má»i request | JWT middleware reject | Kiá»ƒm tra auth-service gRPC connection. Verify token format. Kiá»ƒm tra clock sync |
| 7 | Routing sai â€” request Ä‘áº¿n sai service | Route config sai | Kiá»ƒm tra route mapping trong gateway config. Verify: `/api/equipment` â†’ equipment-service:8082 |

### 10.2. Debug API Gateway

```bash
# Health check gateway
curl -s http://localhost:8080/health | jq .

# Test routing
curl -v http://localhost:8080/api/equipment 2>&1 | grep "< HTTP"
# Mong Ä‘á»£i: < HTTP/1.1 200 OK (náº¿u cÃ³ token) hoáº·c 401 (náº¿u khÃ´ng)

# Kiá»ƒm tra CORS headers
curl -v -X OPTIONS http://localhost:8080/api/equipment \
  -H "Origin: http://localhost:4200" \
  -H "Access-Control-Request-Method: GET" 2>&1 | grep -i "access-control"

# Xem Nginx logs (náº¿u gateway dÃ¹ng Nginx)
docker exec maintenix-gateway tail -50 /var/log/nginx/access.log
docker exec maintenix-gateway tail -50 /var/log/nginx/error.log
```

---

## 11. Frontend â€” Angular

### 11.1. Build & Compile Errors

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `npm install` fail vá»›i `ERESOLVE` | Dependency version conflict | `npm install --legacy-peer-deps`. Hoáº·c xÃ³a `node_modules` + `package-lock.json` rá»“i `npm install` |
| 2 | `Module not found: Error: Can't resolve 'ng-zorro-antd/...'` | ng-zorro chÆ°a cÃ i hoáº·c version sai | `npm install ng-zorro-antd@17.3.0`. Kiá»ƒm tra version match vá»›i Angular |
| 3 | `error TS2307: Cannot find module` | Path sai hoáº·c file chÆ°a táº¡o | Kiá»ƒm tra `tsconfig.json` paths config. Verify import Ä‘Ãºng relative path |
| 4 | `Expression has changed after it was checked` | Change detection race condition | DÃ¹ng `ChangeDetectorRef.detectChanges()` hoáº·c wrap trong `setTimeout`. Hoáº·c dÃ¹ng `OnPush` strategy |
| 5 | `ng serve` cháº­m (> 30 giÃ¢y) | QuÃ¡ nhiá»u files hoáº·c thiáº¿u cache | XÃ³a `.angular/cache/` rá»“i cháº¡y láº¡i. Kiá»ƒm tra `node_modules` corruption |
| 6 | `Allocation failed - JavaScript heap out of memory` | Build cáº§n nhiá»u RAM | `node --max-old-space-size=8192 ./node_modules/.bin/ng build` |
| 7 | Hot reload khÃ´ng hoáº¡t Ä‘á»™ng | File watcher limit hoáº·c poll mode | WSL2: tÄƒng inotify limit: `echo fs.inotify.max_user_watches=524288 \| sudo tee -a /etc/sysctl.conf`. Angular: `ng serve --poll 2000` |
| 8 | SCSS compile error | Syntax sai hoáº·c thiáº¿u import | Kiá»ƒm tra `@import` paths trong `styles.scss`. ng-zorro: `@import "ng-zorro-antd/ng-zorro-antd.min.css"` |

### 11.2. Runtime Errors (Browser)

| # | Lá»—i (Console) | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `HttpErrorResponse: Http failure response for http://localhost:8080/api/...: 0 Unknown Error` | CORS block hoáº·c backend down | F12 Network tab â†’ kiá»ƒm tra request. Backend: verify CORS config cho `localhost:4200` |
| 2 | `NullInjectorError: No provider for HttpClient` | `HttpClientModule` chÆ°a import | ThÃªm `provideHttpClient()` trong `app.config.ts` hoáº·c import `HttpClientModule` |
| 3 | `Cannot read properties of undefined (reading 'xxx')` | Data chÆ°a load xong (async) | DÃ¹ng optional chaining: `data?.property`. Hoáº·c `*ngIf="data"` trong template |
| 4 | `NG0100: ExpressionChangedAfterItHasBeenChecked` | Template binding thay Ä‘á»•i ngoÃ i Angular zone | Wrap trong `NgZone.run()` hoáº·c `ChangeDetectorRef.detectChanges()` |
| 5 | `ERROR TypeError: this.xxx is not a function` | Service method undefined hoáº·c scope sai | Kiá»ƒm tra injection. Äáº£m báº£o service cÃ³ `@Injectable({providedIn: 'root'})` |
| 6 | Trang tráº¯ng sau login | Route guard redirect loop hoáº·c lazy loading fail | F12 Console â†’ check error. Verify routes trong `app.routes.ts`. Kiá»ƒm tra AuthGuard logic |
| 7 | ECharts khÃ´ng render | Container div chÆ°a cÃ³ kÃ­ch thÆ°á»›c | Äáº£m báº£o parent container cÃ³ `height` (vd: `style="height: 400px"`). `echarts.resize()` khi window resize |
| 8 | ng-zorro component khÃ´ng hiá»ƒn thá»‹ | Module chÆ°a import hoáº·c thiáº¿u CSS | Kiá»ƒm tra import module tÆ°Æ¡ng á»©ng (vd: `NzTableModule`). Verify `styles.scss` cÃ³ import ng-zorro CSS |
| 9 | Routing: `Cannot match any routes` | URL sai hoáº·c route chÆ°a config | Kiá»ƒm tra `app.routes.ts`. Verify lazy loading path Ä‘Ãºng. ThÃªm wildcard route `**` â†’ redirect |
| 10 | API tráº£ 401 liÃªn tá»¥c sau vÃ i phÃºt | Token háº¿t háº¡n, chÆ°a implement refresh | Implement `AuthInterceptor` vá»›i token refresh logic. Hoáº·c tÄƒng token TTL (dev mode) |

### 11.3. Debug Frontend

```bash
# Cháº¡y vá»›i verbose output
ng serve --verbose

# Build production (check for errors)
ng build --configuration production 2>&1 | head -50

# Kiá»ƒm tra bundle size
ng build --configuration production --stats-json
npx webpack-bundle-analyzer dist/maintenix-app/stats.json

# Lint kiá»ƒm tra
npx eslint src/ --ext .ts
npx prettier --check "src/**/*.{ts,html,scss}"
```

**Browser DevTools Tips:**

```
1. F12 â†’ Console tab: Xem Angular errors, API errors
2. F12 â†’ Network tab: Kiá»ƒm tra API calls, status codes, response body
3. F12 â†’ Application tab: Kiá»ƒm tra localStorage (auth token)
4. F12 â†’ Performance tab: Profile rendering performance
5. Angular DevTools extension: Component tree, change detection profiling
```

---

## 12. Authentication & Authorization (RBAC)

### 12.1. Auth Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Login fail: `AUTH_INVALID_CREDENTIALS` | Sai username/password | Dev accounts: admin/123456, engineer/123456, viewer/123456. Check mock data |
| 2 | `AUTH_TOKEN_EXPIRED` (401) | JWT háº¿t háº¡n (default 24h) | Frontend: implement token refresh. Backend: tÄƒng token TTL cho dev |
| 3 | `AUTH_TOKEN_INVALID` (401) | JWT bá»‹ tamper hoáº·c sai signing key | Kiá»ƒm tra JWT secret/RSA key Ä‘á»“ng nháº¥t giá»¯a auth-service vÃ  gateway |
| 4 | `AUTH_INSUFFICIENT_ROLE` (403) | User role khÃ´ng Ä‘á»§ quyá»n | Kiá»ƒm tra RBAC matrix. Verify user role trong JWT claims. Xem `rbac-matrix.md` |
| 5 | `AUTH_ACCOUNT_LOCKED` (403) | Login sai quÃ¡ 5 láº§n | Reset: `UPDATE users SET failed_login_count=0, locked_until=NULL WHERE username='xxx'` |
| 6 | `AUTH_TOO_MANY_ATTEMPTS` (429) | Rate limit login endpoint | Äá»£i cooldown (thÆ°á»ng 5 phÃºt). Hoáº·c xÃ³a rate limit key trong Redis |
| 7 | Token khÃ´ng truyá»n lÃªn backend | AuthInterceptor chÆ°a cÃ i hoáº·c chÆ°a attach header | Kiá»ƒm tra interceptor: `Authorization: Bearer <token>`. F12 Network tab verify header |
| 8 | Sau logout váº«n truy cáº­p Ä‘Æ°á»£c | JWT chÆ°a thÃªm vÃ o blacklist | Backend: implement JWT blacklist trong Redis. Frontend: xÃ³a token khá»i localStorage |

### 12.2. Debug Auth

```bash
# Decode JWT token (khÃ´ng cáº§n secret)
echo "eyJhbGciOi..." | cut -d '.' -f 2 | base64 -d 2>/dev/null | jq .
# â†’ Xem payload: userId, role, exp, permissions

# Kiá»ƒm tra token qua gRPC
grpcurl -plaintext -d '{"token":"eyJhbGciOi..."}' \
  localhost:50051 auth.v1.AuthService/ValidateToken

# Kiá»ƒm tra JWT blacklist trong Redis
docker exec maintenix-redis redis-cli SISMEMBER "jwt:blacklist" "<token-hash>"

# Kiá»ƒm tra user role trong DB
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT id, username, role, status, failed_login_count FROM users;"

# Kiá»ƒm tra RBAC permissions
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT role, resource, actions FROM rbac_policies ORDER BY role;"
```

### 12.3. RBAC Role Quick Reference

```
super_admin         â†’ Full access táº¥t cáº£
factory_manager     â†’ View all + manage equipment, maintenance, reports
maintenance_manager â†’ Manage work orders, maintenance schedules, spare parts
maintenance_engineerâ†’ Create/edit work orders, equipment
quality_inspector   â†’ Verify completed work orders
technician          â†’ Update assigned work orders, view equipment
data_scientist      â†’ Manage AI models, pipelines
viewer              â†’ Read-only dashboard, equipment, alerts
```

---

## 13. WebSocket & Real-time Data

### 13.1. WebSocket Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | WebSocket connect fail: `ws://localhost:8080/ws` | Gateway chÆ°a config WebSocket proxy | Nginx: thÃªm `proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection "upgrade";` |
| 2 | WebSocket ngáº¯t káº¿t ná»‘i liÃªn tá»¥c | Timeout hoáº·c proxy disconnect | TÄƒng `proxy_read_timeout` (Nginx). Implement ping/pong heartbeat (má»—i 30s) |
| 3 | KhÃ´ng nháº­n Ä‘Æ°á»£c alert real-time | WebSocket subscription sai topic | Verify: subscribe `/topic/factory-alerts`. Kiá»ƒm tra alert-service Kafka consumer running |
| 4 | Alert bá»‹ duplicate trÃªn UI | WebSocket reconnect re-subscribe | Frontend: dedup theo alert ID. Hoáº·c unsubscribe trÆ°á»›c khi reconnect |
| 5 | WebSocket lag (delay > 5 giÃ¢y) | Kafka consumer lag hoáº·c processing cháº­m | Kiá»ƒm tra Kafka consumer lag (section 7). Optimize alert processing pipeline |

### 13.2. Debug WebSocket

```bash
# Test WebSocket connection (cáº§n wscat)
npm install -g wscat
wscat -c "ws://localhost:8080/ws"
# â†’ Gá»­i: {"type":"subscribe","topic":"factory-alerts"}

# Kiá»ƒm tra WebSocket connections trong alert-service
curl -s http://localhost:8084/debug/websocket/connections | jq .

# Simulate alert event (trigger WebSocket broadcast)
curl -s -X POST http://localhost:8084/api/alerts \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "equipmentId": "EQ001",
    "sensorId": "S001",
    "severity": "critical",
    "message": "Test alert"
  }'
```

---

## 14. Sensor Pipeline & Anomaly Detection

### 14.1. Sensor Pipeline Flow

```
PLC/SCADA â†’ opcua-bridge â†’ Kafka (sensor.raw) â†’ sensor-service â†’ Kafka (sensor.processed)
                                                       â†“
                                              InfluxDB + TimescaleDB
                                                       â†“
                                               alert-service (anomaly check)
                                                       â†“
                                              Kafka (alert.created) â†’ WebSocket â†’ Frontend
```

### 14.2. Pipeline Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Sensor data khÃ´ng xuáº¥t hiá»‡n trÃªn Dashboard | Pipeline bá»‹ Ä‘á»©t á»Ÿ bÆ°á»›c nÃ o Ä‘Ã³ | Kiá»ƒm tra tá»«ng bÆ°á»›c theo flow á»Ÿ trÃªn. Xem Kafka topics cÃ³ message khÃ´ng |
| 2 | Anomaly detection khÃ´ng trigger alert | Threshold config sai hoáº·c sensor data quality=bad | Kiá»ƒm tra sensor thresholds trong DB. Verify data quality flag |
| 3 | Health Score khÃ´ng cáº­p nháº­t | Sensor service khÃ´ng ghi vÃ o equipment cache | Kiá»ƒm tra Redis: `GET equipment:health:EQ001`. Verify Kafka event `equipment.status` |
| 4 | Dá»¯ liá»‡u sensor bá»‹ trá»… > 5 giÃ¢y | Kafka consumer lag hoáº·c DB write slow | Kiá»ƒm tra consumer lag. Optimize batch write vÃ o InfluxDB/TimescaleDB |
| 5 | GiÃ¡ trá»‹ sensor hiá»ƒn thá»‹ NaN/null | Sensor Ä‘á»c fail hoáº·c parse error | Kiá»ƒm tra DLQ: `maintenix.dlq.sensor.raw`. Verify sensor JSON format |
| 6 | OEE tÃ­nh sai trÃªn Dashboard | Sensor data gaps hoáº·c formula config sai | Verify thá»i gian uptime/downtime. Kiá»ƒm tra OEE calculation logic |

### 14.3. Debug Sensor Pipeline

```bash
# 1. Kiá»ƒm tra Kafka topic sensor.raw cÃ³ data
docker exec maintenix-kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic maintenix.sensor.raw \
  --max-messages 3 --timeout-ms 5000

# 2. Kiá»ƒm tra Kafka topic sensor.processed
docker exec maintenix-kafka kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic maintenix.sensor.processed \
  --max-messages 3 --timeout-ms 5000

# 3. Kiá»ƒm tra InfluxDB cÃ³ data
docker exec maintenix-influxdb influx query \
  'from(bucket:"sensor_realtime") |> range(start: -10m) |> limit(n:5)' \
  --org maintenix-org --token maintenix-influx-token

# 4. Kiá»ƒm tra Redis cache sensor latest
docker exec maintenix-redis redis-cli --scan --pattern "sensor:latest:*" --count 10

# 5. Kiá»ƒm tra alert-service nháº­n sensor events
docker exec maintenix-kafka kafka-consumer-groups.sh \
  --describe --group alert-sensor-processor --bootstrap-server localhost:9092

# 6. Kiá»ƒm tra sensor thresholds
docker exec maintenix-postgres psql -U maintenix -c \
  "SELECT id, equipment_id, type, warning_low, warning_high, critical_low, critical_high FROM sensors WHERE equipment_id='EQ001';"
```

---

## 15. AI/ML Service

### 15.1. ML Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Model prediction tráº£ error | Model chÆ°a load hoáº·c input format sai | Kiá»ƒm tra model status: `GET /api/models`. Verify model file trong MinIO |
| 2 | Pipeline stuck á»Ÿ `RUNNING` | Worker crash hoáº·c timeout | Kiá»ƒm tra logs ml-service. Stop pipeline: `POST /api/pipelines/:id/stop` |
| 3 | `MODEL_NOT_VALIDATED` khi activate | Model chÆ°a qua validation step | Cháº¡y validation: `POST /api/models/:id/validate`. Review validation metrics |
| 4 | Prediction accuracy giáº£m Ä‘á»™t ngá»™t | Data drift hoáº·c model stale | Retrain model vá»›i data má»›i. Monitor data drift metrics trong Grafana |
| 5 | MinIO connection error | MinIO container down hoáº·c credentials sai | Kiá»ƒm tra MinIO: http://localhost:9001. Verify `MINIO_ROOT_USER/PASSWORD` |

---

## 16. Monitoring & Observability

### 16.1. Observability Stack

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚              Observability Stack            â”‚
                    â”‚                                             â”‚
                    â”‚  Metrics:  Prometheus (:9090) â†’ Grafana     â”‚
                    â”‚  Logs:     Container stdout â†’ docker logs   â”‚
                    â”‚  Traces:   Jaeger (:16686)                  â”‚
                    â”‚  Alerts:   Prometheus AlertManager          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 16.2. Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Prometheus khÃ´ng scrape metrics | Target unreachable hoáº·c sai port | http://localhost:9090/targets â†’ kiá»ƒm tra status. Verify `prometheus.yml` config |
| 2 | Grafana dashboard trá»‘ng | Datasource chÆ°a config hoáº·c query sai | Grafana â†’ Data Sources â†’ thÃªm Prometheus URL: `http://prometheus:9090` |
| 3 | Jaeger khÃ´ng nháº­n traces | Service chÆ°a cáº¥u hÃ¬nh tracing | Kiá»ƒm tra `JAEGER_AGENT_HOST` trong service config. Verify Jaeger UI: http://localhost:16686 |
| 4 | Prometheus disk full | Retention quÃ¡ lá»›n | Set retention: `--storage.tsdb.retention.time=15d --storage.tsdb.retention.size=5GB` |
| 5 | Grafana login fail | Password Ä‘Ã£ Ä‘á»•i | Default: admin/admin. Reset: `docker exec maintenix-grafana grafana-cli admin reset-admin-password admin` |

### 16.3. Key Metrics cáº§n Monitor

```bash
# Prometheus PromQL queries há»¯u Ã­ch:

# Request rate per service
rate(http_requests_total[5m])

# Error rate (5xx)
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Request latency P95
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Kafka consumer lag
kafka_consumer_lag_sum

# Active database connections
pg_stat_activity_count

# Redis memory usage
redis_memory_used_bytes

# Go goroutine count (memory leak indicator)
go_goroutines

# DLQ message count
kafka_dlq_messages_total
```

---

## 17. Performance & Tá»‘i Æ°u

### 17.1. Váº¥n Ä‘á» Performance

| # | Triá»‡u chá»©ng | NguyÃªn nhÃ¢n cÃ³ thá»ƒ | CÃ¡ch cháº©n Ä‘oÃ¡n & fix |
|---|-------------|-------------------|----------------------|
| 1 | API response > 2 giÃ¢y | DB query cháº­m, missing index | Backend: thÃªm middleware log query time. DB: `EXPLAIN ANALYZE` â†’ thÃªm index |
| 2 | Frontend load cháº­m (> 5 giÃ¢y) | Bundle quÃ¡ lá»›n, quÃ¡ nhiá»u API calls | `ng build --stats-json` â†’ analyze bundle. Implement lazy loading cho modules |
| 3 | RAM tÄƒng liÃªn tá»¥c (Go service) | Goroutine leak, connection pool leak | `go tool pprof` heap profile. Kiá»ƒm tra goroutine count trending |
| 4 | Docker container OOMKilled | Memory limit quÃ¡ tháº¥p | `docker stats` kiá»ƒm tra usage. TÄƒng `mem_limit` trong docker-compose |
| 5 | Kafka throughput tháº¥p | Partition Ã­t, batch size nhá» | TÄƒng partitions. Tune `batch.size`, `linger.ms` cho producer |
| 6 | Dashboard render cháº­m | ECharts render quÃ¡ nhiá»u data points | Downsample data: `GROUP BY time(5m)` thay vÃ¬ raw points. Virtualize tables |
| 7 | Sensor pipeline lag > 10 giÃ¢y | sensor-service xá»­ lÃ½ cháº­m | TÄƒng consumer instances (horizontal scale). Optimize processing logic |

### 17.2. Performance Benchmarks â€” Má»¥c tiÃªu

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                            â”‚ Target       â”‚ Critical      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ API response time (P95)           â”‚ < 500ms      â”‚ > 2000ms      â”‚
â”‚ Sensor ingestion latency          â”‚ < 1s         â”‚ > 5s          â”‚
â”‚ Sensor â†’ Alert pipeline           â”‚ < 5s         â”‚ > 30s         â”‚
â”‚ Frontend initial load (LCP)       â”‚ < 3s         â”‚ > 8s          â”‚
â”‚ Frontend route navigation         â”‚ < 500ms      â”‚ > 2000ms      â”‚
â”‚ WebSocket event delivery          â”‚ < 2s         â”‚ > 10s         â”‚
â”‚ Database query (simple)           â”‚ < 50ms       â”‚ > 500ms       â”‚
â”‚ Database query (complex report)   â”‚ < 2s         â”‚ > 10s         â”‚
â”‚ Kafka produce latency             â”‚ < 100ms      â”‚ > 1000ms      â”‚
â”‚ Redis cache hit                   â”‚ < 5ms        â”‚ > 50ms        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 18. Networking & Port Conflicts

### 18.1. Port Map â€” Táº¥t cáº£ Services

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Port  â”‚ Service                  â”‚ Protocol    â”‚ Ghi chÃº              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 4200  â”‚ Angular Dev Server       â”‚ HTTP        â”‚ Frontend             â”‚
â”‚ 8080  â”‚ API Gateway              â”‚ HTTP/WS     â”‚ Main entry point     â”‚
â”‚ 8081  â”‚ auth-service             â”‚ HTTP+gRPC   â”‚ gRPC: 50051          â”‚
â”‚ 8082  â”‚ equipment-service        â”‚ HTTP+gRPC   â”‚ gRPC: 50052          â”‚
â”‚ 8083  â”‚ sensor-service           â”‚ HTTP+gRPC   â”‚ gRPC: 50053          â”‚
â”‚ 8084  â”‚ alert-service            â”‚ HTTP+WS     â”‚ WebSocket + gRPC     â”‚
â”‚ 8085  â”‚ workorder-service        â”‚ HTTP        â”‚                      â”‚
â”‚ 8086  â”‚ ml-service / InfluxDB    â”‚ HTTP+gRPC   â”‚ âš ï¸ Port conflict!   â”‚
â”‚ 8087  â”‚ notification-service     â”‚ Kafka only  â”‚ No HTTP              â”‚
â”‚ 4840  â”‚ opcua-bridge             â”‚ OPC-UA      â”‚ PLC/SCADA            â”‚
â”‚ 5432  â”‚ PostgreSQL               â”‚ TCP         â”‚ Master data          â”‚
â”‚ 5433  â”‚ TimescaleDB              â”‚ TCP         â”‚ Time-series          â”‚
â”‚ 6379  â”‚ Redis                    â”‚ TCP         â”‚ Cache + Sessions     â”‚
â”‚ 8200  â”‚ Vault                    â”‚ HTTP        â”‚ Secrets              â”‚
â”‚ 9000  â”‚ MinIO API                â”‚ HTTP        â”‚ S3 storage           â”‚
â”‚ 9001  â”‚ MinIO Console            â”‚ HTTP        â”‚ Web UI               â”‚
â”‚ 9090  â”‚ Prometheus               â”‚ HTTP        â”‚ Metrics              â”‚
â”‚ 9092  â”‚ Kafka Broker             â”‚ TCP         â”‚ Event streaming      â”‚
â”‚ 9093  â”‚ Kafka UI                 â”‚ HTTP        â”‚ Web UI               â”‚
â”‚ 3000  â”‚ Grafana                  â”‚ HTTP        â”‚ Dashboards           â”‚
â”‚ 16686 â”‚ Jaeger                   â”‚ HTTP        â”‚ Tracing UI           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 18.2. Kiá»ƒm tra & Fix Port Conflicts

**Windows (PowerShell):**

```powershell
# Kiá»ƒm tra táº¥t cáº£ ports
$ports = @(4200,8080,8081,8082,8083,8084,8085,8086,8087,4840,
           5432,5433,6379,8200,9000,9001,9090,9092,9093,3000,16686)

foreach ($port in $ports) {
    $conn = Get-NetTCPConnection -LocalPort $port -ErrorAction SilentlyContinue
    if ($conn) {
        $proc = Get-Process -Id $conn[0].OwningProcess -ErrorAction SilentlyContinue
        Write-Host "âš ï¸  Port $port â†’ PID $($conn[0].OwningProcess) ($($proc.ProcessName))" -ForegroundColor Yellow
    }
}

# Kill process Ä‘ang chiáº¿m port
Stop-Process -Id <PID> -Force
```

**Linux / macOS / WSL:**

```bash
# Kiá»ƒm tra port
lsof -i :8080
# hoáº·c
ss -tlnp | grep :8080

# Kill process
kill -9 $(lsof -t -i :8080)
```

### 18.3. Lá»—i Networking thÆ°á»ng gáº·p

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Service A (Docker) khÃ´ng káº¿t ná»‘i service B (host) | Docker container dÃ¹ng `localhost` â†’ trá» vá» chÃ­nh container | DÃ¹ng `host.docker.internal` thay vÃ¬ `localhost` (Docker Desktop) |
| 2 | Service A (host) khÃ´ng káº¿t ná»‘i service B (Docker) | Port chÆ°a expose | Kiá»ƒm tra `docker compose ps` â†’ cá»™t PORTS. ThÃªm port mapping náº¿u thiáº¿u |
| 3 | Docker container khÃ´ng cÃ³ internet | DNS resolve fail | Docker Desktop â†’ Settings â†’ Docker Engine â†’ thÃªm `"dns": ["8.8.8.8"]` |
| 4 | `ml-service` vÃ  `InfluxDB` cÃ¹ng port 8086 | Port conflict khi cháº¡y cáº£ 2 trÃªn host | Äá»•i ml-service sang port khÃ¡c (vd: 8096). Hoáº·c cháº¡y ml-service trong Docker |

---

## 19. Data & Migration Issues

### 19.1. Migration Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | `dirty database version X` | Migration trÆ°á»›c fail giá»¯a chá»«ng | Force version: `migrate -path migrations -database $DB_URL force <X>`. Rá»“i fix migration SQL vÃ  cháº¡y láº¡i |
| 2 | `migration file not found` | Thiáº¿u file migration | Kiá»ƒm tra thÆ° má»¥c `migrations/`. Ensure cáº£ file `.up.sql` vÃ  `.down.sql` tá»“n táº¡i |
| 3 | `already exists` khi migrate up | Migration Ä‘Ã£ cháº¡y trÆ°á»›c Ä‘Ã³ | Skip migration Ä‘Ã£ cháº¡y: kiá»ƒm tra `schema_migrations` table. Hoáº·c `migrate force <version>` |
| 4 | Seed data khÃ´ng load | Script lá»—i hoáº·c table chÆ°a táº¡o | Cháº¡y `migrate up` trÆ°á»›c â†’ rá»“i má»›i `make seed`. Kiá»ƒm tra seed script logs |
| 5 | Foreign key constraint error khi seed | Thá»© tá»± insert sai (parent trÆ°á»›c child) | Kiá»ƒm tra thá»© tá»±: users â†’ equipment â†’ sensors â†’ ... Táº¯t FK check táº¡m náº¿u cáº§n |

### 19.2. Data Issues

| # | Lá»—i | NguyÃªn nhÃ¢n | CÃ¡ch fix |
|---|------|-------------|----------|
| 1 | Dá»¯ liá»‡u frontend khÃ´ng khá»›p mock-data.md | Mock data trong code outdated | Sync `src/app/core/mock/mock-data.ts` vá»›i `mock-data.md` documentation |
| 2 | Sensor time-series bá»‹ gaps | Sensor offline hoáº·c ingestion fail | Kiá»ƒm tra sensor status. Check DLQ cho failed messages. Review uptime logs |
| 3 | Work order count sai trÃªn dashboard | Stale cache hoáº·c query filter sai | Invalidate cache: `redis-cli DEL dashboard:kpi`. Review query logic |
| 4 | Equipment health score = 0 máº·c dÃ¹ sensors OK | Health calculator bug hoáº·c missing sensor mapping | Kiá»ƒm tra sensor â†’ equipment mapping trong DB. Verify health calculation formula |

### 19.3. Reset Data hoÃ n toÃ n

```bash
# âš ï¸ CHá»ˆ DÃ™NG CHO DEVELOPMENT â€” Máº¤T TOÃ€N Bá»˜ DATA

# 1. Dá»«ng táº¥t cáº£ services
docker compose -f docker-compose.yml down

# 2. XÃ³a volumes (data)
docker compose -f docker-compose.infra.yml down -v

# 3. Khá»Ÿi Ä‘á»™ng láº¡i infrastructure
docker compose -f docker-compose.infra.yml up -d

# 4. Äá»£i healthy
sleep 30
docker compose -f docker-compose.infra.yml ps

# 5. Cháº¡y migrations
make migrate-up

# 6. Seed demo data
make seed

# 7. Khá»Ÿi Ä‘á»™ng backend services
make dev-all

# 8. Verify
curl -s http://localhost:8080/health | jq .
```

---

## 20. Production Issues

### 20.1. Nhá»¯ng gÃ¬ khÃ¡c biá»‡t so vá»›i Development

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ KhÃ­a cáº¡nh           â”‚ Development             â”‚ Production               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Database            â”‚ Docker container        â”‚ Managed RDS/CloudSQL     â”‚
â”‚ Kafka               â”‚ Single broker           â”‚ 3+ brokers (cluster)     â”‚
â”‚ Redis               â”‚ Single instance         â”‚ Redis Cluster/Sentinel   â”‚
â”‚ Secrets             â”‚ .env file               â”‚ Vault / K8s Secrets      â”‚
â”‚ SSL/TLS             â”‚ KhÃ´ng                   â”‚ Báº¯t buá»™c (cert-manager)  â”‚
â”‚ Scaling             â”‚ 1 instance/service      â”‚ HPA (auto-scale)         â”‚
â”‚ Logging             â”‚ stdout                  â”‚ ELK/Loki centralized     â”‚
â”‚ Monitoring          â”‚ Local Prometheus        â”‚ Prometheus + AlertManager â”‚
â”‚ JWT signing         â”‚ HS256 (shared secret)   â”‚ RS256 (RSA key pair)     â”‚
â”‚ CORS                â”‚ localhost:4200          â”‚ production domain only   â”‚
â”‚ Rate limiting       â”‚ Cao (dev-friendly)      â”‚ Strict per-user limits   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 20.2. Production Checklist â€” TrÆ°á»›c khi Deploy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Production Deployment Checklist                      â”‚
â”‚                                                                         â”‚
â”‚  Security:                                                              â”‚
â”‚  â–¡  Táº¥t cáº£ secrets trong Vault / K8s Secrets (khÃ´ng hardcode)           â”‚
â”‚  â–¡  JWT signing dÃ¹ng RS256 (khÃ´ng HS256)                                â”‚
â”‚  â–¡  CORS chá»‰ allow production domain                                    â”‚
â”‚  â–¡  Rate limiting Ä‘Ã£ enable cho táº¥t cáº£ endpoints                        â”‚
â”‚  â–¡  HTTPS/TLS certificates valid                                        â”‚
â”‚  â–¡  Database credentials rotated                                        â”‚
â”‚  â–¡  Debug endpoints (pprof) disabled                                    â”‚
â”‚                                                                         â”‚
â”‚  Database:                                                              â”‚
â”‚  â–¡  Migrations Ä‘Ã£ cháº¡y trÃªn production DB                               â”‚
â”‚  â–¡  Backup strategy configured (daily)                                  â”‚
â”‚  â–¡  Connection pool tuned                                               â”‚
â”‚  â–¡  Indexes Ä‘Ã£ táº¡o cho production queries                               â”‚
â”‚                                                                         â”‚
â”‚  Infrastructure:                                                        â”‚
â”‚  â–¡  Kafka replication factor â‰¥ 3                                        â”‚
â”‚  â–¡  Redis Sentinel / Cluster enabled                                    â”‚
â”‚  â–¡  Resource limits set cho táº¥t cáº£ containers                           â”‚
â”‚  â–¡  Health checks configured                                            â”‚
â”‚  â–¡  HPA (auto-scaling) configured                                       â”‚
â”‚                                                                         â”‚
â”‚  Monitoring:                                                            â”‚
â”‚  â–¡  Prometheus alerting rules active                                    â”‚
â”‚  â–¡  Grafana dashboards configured                                       â”‚
â”‚  â–¡  DLQ monitoring enabled                                              â”‚
â”‚  â–¡  Error rate alerting (Slack/PagerDuty)                               â”‚
â”‚  â–¡  Log aggregation (ELK/Loki) working                                  â”‚
â”‚                                                                         â”‚
â”‚  Application:                                                           â”‚
â”‚  â–¡  Environment variables set cho production                            â”‚
â”‚  â–¡  Log level = warn (khÃ´ng debug)                                      â”‚
â”‚  â–¡  Graceful shutdown implemented                                       â”‚
â”‚  â–¡  Circuit breakers configured                                         â”‚
â”‚  â–¡  Frontend build with --configuration production                      â”‚
â”‚  â–¡  Source maps disabled (hoáº·c uploaded to Sentry)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 20.3. Incident Response Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Alert      â”‚â”€â”€â”€â”€â–¶â”‚  Acknowledge      â”‚â”€â”€â”€â”€â–¶â”‚  Investigate     â”‚
â”‚  triggered  â”‚      â”‚  (within 5 min)   â”‚     â”‚  (check logs,    â”‚
â”‚  (PagerDuty/â”‚      â”‚                   â”‚     â”‚   metrics,       â”‚
â”‚   Slack)    â”‚      â”‚                   â”‚     â”‚   traces)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Post-mortem      â”‚â—€â”€â”€â”€â”‚  Mitigate         â”‚
                    â”‚  (RCA + action    â”‚     â”‚  (fix/rollback/   â”‚
                    â”‚   items)          â”‚     â”‚   scale)          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Khi nÃ o rollback:**

```
Rollback ngay láº­p tá»©c náº¿u:
  Â· Error rate > 5% trong 5 phÃºt liÃªn tá»¥c
  Â· P95 latency > 5x bÃ¬nh thÆ°á»ng
  Â· Critical service (auth, alert) completely down
  Â· Data corruption detected

Lá»‡nh rollback:
  kubectl rollout undo deployment/<service> -n maintenix-prod
  # Hoáº·c
  make k8s-rollback SVC=<service>
```

---

## 21. Quick Reference â€” Lá»‡nh Cháº©n Ä‘oÃ¡n

### 21.1. One-liner Health Check

```bash
# Táº¥t cáº£ trong 1 lá»‡nh
echo "=== Docker ===" && docker compose -f docker-compose.infra.yml ps --format "table {{.Name}}\t{{.Status}}" && \
echo "=== Backend ===" && for p in 8081 8082 8083 8084 8085 8086 8087; do echo -n ":$p â†’ "; curl -s --max-time 2 http://localhost:$p/health | jq -r '.status' 2>/dev/null || echo "DOWN"; done && \
echo "=== Frontend ===" && curl -s --max-time 2 http://localhost:4200 > /dev/null && echo ":4200 â†’ UP" || echo ":4200 â†’ DOWN"
```

### 21.2. Lá»‡nh Debug theo ThÃ nh pháº§n

```bash
# â”€â”€â”€ Docker â”€â”€â”€
docker compose ps                              # Tráº¡ng thÃ¡i containers
docker compose logs -f --tail=50 <service>     # Logs real-time
docker stats --no-stream                       # Resource usage
docker system df                               # Disk usage

# â”€â”€â”€ PostgreSQL â”€â”€â”€
docker exec maintenix-postgres psql -U maintenix -c "\dt"              # Tables
docker exec maintenix-postgres psql -U maintenix -c "SELECT version();"  # Version

# â”€â”€â”€ Redis â”€â”€â”€
docker exec maintenix-redis redis-cli PING                            # Health
docker exec maintenix-redis redis-cli INFO memory                     # Memory
docker exec maintenix-redis redis-cli DBSIZE                          # Key count

# â”€â”€â”€ Kafka â”€â”€â”€
docker exec maintenix-kafka kafka-topics.sh --list --bootstrap-server localhost:9092
docker exec maintenix-kafka kafka-consumer-groups.sh --list --bootstrap-server localhost:9092

# â”€â”€â”€ Backend â”€â”€â”€
curl -s http://localhost:8080/health | jq .                           # Gateway health
curl -s http://localhost:8082/api/equipment -H "Authorization: Bearer $TOKEN" | jq .

# â”€â”€â”€ Frontend â”€â”€â”€
ng serve --verbose 2>&1 | head -20                                     # Build output
npx eslint src/ --ext .ts 2>&1 | tail -5                              # Lint errors

# â”€â”€â”€ gRPC â”€â”€â”€
grpcurl -plaintext localhost:50051 list                                # Services
grpcurl -plaintext localhost:50051 describe auth.v1.AuthService        # Methods
```

### 21.3. Emergency Commands

```bash
# ğŸ”´ Restart táº¥t cáº£ infrastructure
docker compose -f docker-compose.infra.yml restart

# ğŸ”´ Restart 1 service cá»¥ thá»ƒ
docker compose restart <service-name>

# ğŸ”´ Kill & recreate 1 container
docker compose up -d --force-recreate <service-name>

# ğŸ”´ Reset toÃ n bá»™ (Máº¤T DATA)
docker compose -f docker-compose.infra.yml down -v && docker compose -f docker-compose.infra.yml up -d

# ğŸ”´ Flush Redis cache (users pháº£i login láº¡i)
docker exec maintenix-redis redis-cli FLUSHALL

# ğŸ”´ Force kill táº¥t cáº£ Docker containers
docker kill $(docker ps -q)

# ğŸ”´ Production: Rollback deployment
kubectl rollout undo deployment/<service> -n maintenix-prod

# ğŸ”´ Production: Scale to 0 (emergency stop 1 service)
kubectl scale deployment/<service> --replicas=0 -n maintenix-prod
```

### 21.4. Log Patterns â€” TÃ¬m nhanh nguyÃªn nhÃ¢n

```bash
# TÃ¬m errors trong logs
docker compose logs --tail=200 | grep -i "error\|panic\|fatal\|fail"

# TÃ¬m slow queries
docker compose logs equipment-service --tail=500 | grep -i "slow\|duration.*[0-9]\{4,\}ms"

# TÃ¬m connection issues
docker compose logs --tail=200 | grep -i "refused\|timeout\|unreachable\|dial"

# TÃ¬m auth issues
docker compose logs auth-service --tail=200 | grep -i "unauthorized\|forbidden\|expired\|invalid.*token"

# TÃ¬m Kafka issues
docker compose logs --tail=200 | grep -i "kafka\|consumer\|producer\|offset\|rebalance"

# TÃ¬m memory issues
docker compose logs --tail=200 | grep -i "oom\|out of memory\|heap\|alloc"
```

---

